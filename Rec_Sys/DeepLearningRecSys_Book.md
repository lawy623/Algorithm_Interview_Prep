# 深度学习推荐系统
## by 王喆

---
# 目录
#### [1.推荐系统简介](#1)
#### [2.前深度学习时代前的推荐系统](#2)
#### [3.深度学习在推荐系统中的应用](#3)

---

### 代码块
```name
l = []                      ## Initial a empty list
```

# <h2 id="1">推荐系统简介</h2>

### 问题及总结
> 推荐系统在解决什么问题

	用户角度：如何解决信息过载问题，让用户高效获取感兴趣的信息（与搜索系统相对应）
	公司角度：如何更大限度的吸引用户，提升使用时间，提升转化率，从而达到公司商业目标
	两个方向的目标通常来说是一致有共性的。
    
    推荐系统处理的是人(U)在特定场景(C)下与信息(I)之间的关系 -> F(U, C, I)

>  推荐系统基本架构

	数据部分：
		1. "客户端实时数据处理"， "流平台准实时数据处理"， "离线数据处理"
		2. 样本数据供算法使用；数据生成特征；生成系统监控，商业统计数据
	模型部分：
		1. 召回层：高效从海量数据中找到用户感兴趣的
		2. 排序层：通过精排对初筛结果进行排序
		3. 策略层：通过多样性/新鲜性/流行性等指标进行调整
		4. 离线评估与在线a/b test的存在
		5. 离线训练与在线更新

> 深度学习的贡献

	· 对数据的强拟合能力，对特征组合的强挖掘能力
	· 模型的灵活性能适应不同的应用场景和业务数据

---

# <h2 id="2">前深度学习时代前的推荐系统</h2>

### 协同过滤 CF
> 1. 传统方法的优势

	· 可解释性强
	· 硬件要求低
	· 易于快速训练和部署

> 2. 皮尔森相关系数

	· 对余弦相似度的补充，如果两个向量取值scale不一样，先减去均值(人对所有物品的均值），去除人的评分偏置的影响
	· 也可以减去物品的评分均值，去除物品偏置的影响
	· 含义就是大家对更好的/更坏的判断是一致的
	· 但是似乎相似性未解决真实的评分问题，如(4.1, 4.2 ,4.3)与(0.1, 0.2, 0.3)是强相关的，但是对新物品评分的预测仍需考虑本身的偏置问题。
	即相似性是相对的，但是评分是绝对的。

> 3. UserCF的缺点

	1. 互联网场景下，用户数量通常远大于物品数，保存用户相似度信息有极大的存储开销（ O(n^2) )
	2. 用户数据向量往往是稀疏向量，导致低频场景下不太适用（用户本身评价少，就不好评价新的物品）

> 4. UserCF和ItemCF的使用场景

	1. UserCF更适用于新闻推荐，具备社交特性。用于发现热点，而不是总是看到相关的新闻
	2. ItemCF更适用于更稳定一点的推荐场景，如购物/观影。用户总喜欢看到相同类型的推荐
	3. ItemCF倾向于找相同的类型的物品, UserCF倾向于根据人的相似推荐各种不同的东西

> 5. 协同过滤的缺点

	1. 更偏向于热门物品，对稀疏向量的处理不好（热门物品容易对其他的物品相似度高，总是在推荐热门物品）
	2. 无法建模用户特征/物体特征，仅仅用到了用户与物品的交互，泛化性较弱。

### 矩阵分解

使用隐向量建模用户和物品，增加模型的泛化性。

> 1. 如何构建人和物品的隐向量
    
    · 人和物品的共现矩阵为 n x m, 分解为 (n x k) 和 (m x k)两个矩阵相乘的形式， k即为隐向量的长度。
    · k的取值与模型的泛化性+计算开销相关，实际工程中需要平衡
    · 这里的隐向量也只建模了人对物品的评分，没有其他信息
    · 需要求解人对物品的新的评价时，只需要将两个k维向量点积即可
    
> 2. 如何分解矩阵 
    
    · 奇异值分解 （SVD）：用于分解非方形矩阵。
    · M = UZV'. Z为对焦矩阵，记录了奇异值大小
    · 取奇异值最大的k为，分别去UV取前面的k个向量即可
    
> 3. SVD缺陷

    1. SVD要求共现矩阵是稠密的，否则需要对矩阵进行填充
    2. 计算复杂度高达O(mn^2)，不适合互联网的大量数据进行计算
    
> 4. 通过梯度下降获取隐向量

    1. 构建目标函数为用户评分与隐函数点积的差平方，使用梯度下降计算，并且可以加上正则化
    2. min[sum(r_ui - q_i · p_u)^2 + lambda(||q_i||^2 + ||p_u||^2)], 优化的复杂度变为O(mnk)
    3. 正则化目的：使模型权重变小，因此波动性变小，但是损失函数尽可能不受影响
    
> 5. 为什么矩阵分解泛化性好

    · CF算法都是只用了user或者item的向量进行计算，但是矩阵分解是通过优化全局的最优进行的，因为同时用到了用于和物品的信息
    
> 6. 如何消除用户本身对物品打分的偏差

    · 加上偏差系数: r_ui = mu + b_i + b_u + q_i · p_u
    · mu为全局系数，b_i为物品偏差，b_u为用户偏差
    
> 7. 协同过滤的优劣

    · 优势：
        1. 泛化能力强，同时建模了用户物品之间的关联性
        2. 不需要储存O(m^2)或者O(n^2)的向量，只需要O[(m+n)*k]， 降低了一个数量级
        3. 获取的隐向量即为embedding，容易和深度学习相组合
    · 局限性：
        1.并没有加入物品，用户，上下文本身的信息特征，

### 逻辑回归

协同过滤和矩阵分解都是用用户物品矩阵的相似度进行建模，并没有加入别的特征信息。逻辑回归通过将样本进行分类，预测正样本的概率。

> 1. 怎么建模
    
    · 将用户和物品的各种信息当作特征向量x, 与w做点积，进行sigmoid打分(f(x)=1 / (1+e^-(wx+b))).
    · wx的值越大, 得到的概率就越接近1
    
> 2. 如何优化

    · 通过概率建模, p(y|x, w) = (fw(x))^y * (1-fw(x))^(1-y)
    · 根据极大似然, 所有样本的概率最大化为Mul(p(y|x,w)) 对所有样本, 取对数并对w求导更新w
    · sigmoid函数求导为 f'(z) = f(z)*(1-f(z))*z
    · 求导的推导可以看这里, https://zhuanlan.zhihu.com/p/46928319. 梯度为(x(fw(x)-y))对所有样本取平均
    
> 3. logit回归优势

    1. 数学假设符合伯努利公式,可以做二分类的假设
    2. 基本形式是各种特征的加权和, 符合人类的认知, 可解释性强
    
> 4. 局限性

    1. 模型太简单, 无法更有效地进行特征融合, 损失了很多有用的信息


## 特征组合
单一的特征无法会造成信息的损失。

> 1. ploy2通过暴力组合特征加入两两之间的特征交叉，缺点为
    
    1. 原本稀疏的x现在变成了更加稀疏的组合xixj
    2. 权重参数从n个升级为n^2个
    
> 2. FM模型
    
    · 与poly2相比，权重不再是wij，而是两个权重的内积<wi, wj>, 相当于为每个特征学习了隐权重向量，数量从O(n^2) 降到了O(nk)
    · 更好的应对了数据稀疏性问题，因为现在隐函数在任意的样本下都可以学习了
    · 具体算法实现可看 https://www.jianshu.com/p/bb2bce9135e4
    
> 3. FM升级到FFM

    · 引入了特征域感知。每个特征的隐权重向量不是单一个向量，而是一个向量组，当特征与另一个特征相结合时，选取对应的权重向量（有一点像attention）
    · 复杂度提升为O(kn^2), 因为每个特征组里都有n个向量选择
    · 复杂度提升之后最多也只能做2杰特哼
    
> 4. GBDT+LR

    · 利用GBDT构建特征，再用LR进行CTR估计
    · GBDT: 梯度提升决策树
    · 将所有决策树叶子节点组成的特征向量组合起来的到最后的特征。
    · 特征组合能力强劲，而且计算量小，但是训练时间长。但是GBDT容易过拟合，并且丢失了数值特征（只有decision判断了）
    · 从此以后特征工程可以单独进行，只要将得到的特征进行分类即可
    
---

# <h3 id="3">深度学习在推荐系统中的应用</h3>

> 1. 深度学习优势

    · 表达能力更强，更能挖掘数据
    · 模型灵活，容易对业务应用调整
    
> 2. 演化方向
     
    1）改变网络复杂程度
    2) 改变特征交叉方式
    3) 组合模型
    4) FM模型的演化
    5) 注意力机制的应用
    6）序列模型的使用
    7）强化学习的使用