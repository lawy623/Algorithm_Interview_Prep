# 深度学习推荐系统
## by 王喆

---
# 目录
#### [1.推荐系统简介](#1)
#### [2.前深度学习时代前的推荐系统](#2)
#### [3.深度学习在推荐系统中的应用](#3)

---

### 代码块
```name
l = []                      ## Initial a empty list
```

# <h2 id="1">推荐系统简介</h2>

### 问题及总结
> 推荐系统在解决什么问题

	用户角度：如何解决信息过载问题，让用户高效获取感兴趣的信息（与搜索系统相对应）
	公司角度：如何更大限度的吸引用户，提升使用时间，提升转化率，从而达到公司商业目标
	两个方向的目标通常来说是一致有共性的。
    
    推荐系统处理的是人(U)在特定场景(C)下与信息(I)之间的关系 -> F(U, C, I)

>  推荐系统基本架构

	数据部分：
		1. "客户端实时数据处理"， "流平台准实时数据处理"， "离线数据处理"
		2. 样本数据供算法使用；数据生成特征；生成系统监控，商业统计数据
	模型部分：
		1. 召回层：高效从海量数据中找到用户感兴趣的
		2. 排序层：通过精排对初筛结果进行排序
		3. 策略层：通过多样性/新鲜性/流行性等指标进行调整
		4. 离线评估与在线a/b test的存在
		5. 离线训练与在线更新

> 深度学习的贡献

	· 对数据的强拟合能力，对特征组合的强挖掘能力
	· 模型的灵活性能适应不同的应用场景和业务数据

---

# <h2 id="2">前深度学习时代前的推荐系统</h2>

### 协同过滤 CF
> 传统方法的优势

	· 可解释性强
	· 硬件要求低
	· 易于快速训练和部署

> 皮尔森相关系数

	· 对余弦相似度的补充，如果两个向量取值scale不一样，先减去均值(人对所有物品的均值），去除人的评分偏置的影响
	· 也可以减去物品的评分均值，去除物品偏置的影响
	· 含义就是大家对更好的/更坏的判断是一致的
	· 但是似乎相似性未解决真实的评分问题，如(4.1, 4.2 ,4.3)与(0.1, 0.2, 0.3)是强相关的，但是对新物品评分的预测仍需考虑本身的偏置问题。
	即相似性是相对的，但是评分是绝对的。

> UserCF的缺点

	1. 互联网场景下，用户数量通常远大于物品数，保存用户相似度信息有极大的存储开销（ O(n^2) )
	2. 用户数据向量往往是稀疏向量，导致低频场景下不太适用（用户本身评价少，就不好评价新的物品）

> UserCF和ItemCF的使用场景

	1. UserCF更适用于新闻推荐，具备社交特性。用于发现热点，而不是总是看到相关的新闻
	2. ItemCF更适用于更稳定一点的推荐场景，如购物/观影。用户总喜欢看到相同类型的推荐
	3. ItemCF倾向于找相同的类型的物品, UserCF倾向于根据人的相似推荐各种不同的东西

> 协同过滤的缺点

	1. 更偏向于热门物品，对稀疏向量的处理不好（热门物品容易对其他的物品相似度高，总是在推荐热门物品）
	2. 无法建模用户特征/物体特征，仅仅用到了用户与物品的交互，泛化性较弱。

### 矩阵分解

使用隐向量建模用户和物品，增加模型的泛化性。

> 如何构建人和物品的隐向量
    
    · 人和物品的共现矩阵为 n x m, 分解为 (n x k) 和 (m x k)两个矩阵相乘的形式， k即为隐向量的长度。
    · k的取值与模型的泛化性+计算开销相关，实际工程中需要平衡
    · 这里的隐向量也只建模了人对物品的评分，没有其他信息
    · 需要求解人对物品的新的评价时，只需要将两个k维向量点积即可
    
> 如何分解矩阵 
    
    · 奇异值分解 （SVD）：用于分解非方形矩阵。
    · M = UZV'. Z为对焦矩阵，记录了奇异值大小
    · 取奇异值最大的k为，分别去UV取前面的k个向量即可
    
> SVD缺陷

    1. SVD要求共现矩阵是稠密的，否则需要对矩阵进行填充
    2. 计算复杂度高达O(mn^2)，不适合互联网的大量数据进行计算
    
> 通过梯度下降获取隐向量

    1. 构建目标函数为用户评分与隐函数点积的差平方，使用梯度下降计算，并且可以加上正则化
    2. min[sum(r_ui - q_i · p_u)^2 + lambda(||q_i||^2 + ||p_u||^2)], 优化的复杂度变为O(mnk)
    3. 正则化目的：使模型权重变小，因此波动性变小，但是损失函数尽可能不受影响
    
> 为什么矩阵分解泛化性好

    · CF算法都是只用了user或者item的向量进行计算，但是矩阵分解是通过优化全局的最优进行的，因为同时用到了用于和物品的信息
    
> 如何消除用户本身对物品打分的偏差

    · 加上偏差系数: r_ui = mu + b_i + b_u + q_i · p_u
    · mu为全局系数，b_i为物品偏差，b_u为用户偏差
    
> 协同过滤的优劣

    · 优势：
        1. 泛化能力强，同时建模了用户物品之间的关联性
        2. 不需要储存O(m^2)或者O(n^2)的向量，只需要O[(m+n)*k]， 降低了一个数量级
        3. 获取的隐向量即为embedding，容易和深度学习相组合
    · 局限性：
        1.并没有加入物品，用户，上下文本身的信息特征，

### 逻辑回归

协同过滤和矩阵分解都是用用户物品矩阵的相似度进行建模，并没有加入别的特征信息。逻辑回归通过将样本进行分类，预测正样本的概率。

> 怎么建模
    
    · 将用户和物品的各种信息当作特征向量x, 与w做点积，进行sigmoid打分(f(x)=1 / (1+e^-(wx+b))).
    · wx的值越大, 得到的概率就越接近1
    
> 如何优化

    · 通过概率建模, p(y|x, w) = (fw(x))^y * (1-fw(x))^(1-y)。sigmoid建模为softmax的二类形式
    · 根据极大似然, 所有样本的概率最大化为Mul(p(y|x,w)) 对所有样本, 取对数并对w求导更新w
    · sigmoid函数求导为 f'(z) = f(z)*(1-f(z))*z
    · 求导的推导可以看这里, https://zhuanlan.zhihu.com/p/46928319. 梯度为(x(fw(x)-y))对所有样本取平均
    
> logit回归优势

    1. 数学假设符合伯努利公式,可以做二分类的假设
    2. 基本形式是各种特征的加权和, 符合人类的认知, 可解释性强
    
> 局限性

    1. 模型太简单, 无法更有效地进行特征融合, 损失了很多有用的信息


## 特征组合
单一的特征无法会造成信息的损失。

> ploy2通过暴力组合特征加入两两之间的特征交叉，缺点为
    
    1. 原本稀疏的x现在变成了更加稀疏的组合xixj
    2. 权重参数从n个升级为n^2个
    
>  FM模型
    
    · 与poly2相比，权重不再是wij，而是两个权重的内积<wi, wj>, 相当于为每个特征学习了隐权重向量，数量从O(n^2) 降到了O(nk)
    · 更好的应对了数据稀疏性问题，因为现在隐函数在任意的样本下都可以学习了
    · 具体算法实现可看 https://www.jianshu.com/p/bb2bce9135e4
    
> FM升级到FFM

    · 引入了特征域感知。每个特征的隐权重向量不是单一个向量，而是一个向量组，当特征与另一个特征相结合时，选取对应的权重向量（有一点像attention）
    · 复杂度提升为O(kn^2), 因为每个特征组里都有n个向量选择
    · 复杂度提升之后最多也只能做2阶交叉
    
> GBDT+LR

    · 利用GBDT构建特征，再用LR进行CTR估计
    · GBDT: 梯度提升决策树，XGBoost为优化的常用库
    · 将所有决策树叶子节点组成的特征向量组合起来的到最后的特征。
    · 特征组合能力强劲，而且计算量小，但是训练时间长。但是GBDT容易过拟合，并且丢失了数值特征（只有decision判断了）
    · 从此以后特征工程可以单独进行，只要将得到的特征进行分类即可
    
---

# <h3 id="3">深度学习在推荐系统中的应用</h3>

> 深度学习优势

    · 表达能力更强，更能挖掘数据
    · 模型灵活，容易对业务应用调整
    
> 演化方向
     
    1）改变网络复杂程度
    2) 改变特征交叉方式
    3) 组合模型
    4) FM模型的演化
    5) 注意力机制的应用
    6）序列模型的使用
    7）强化学习的使用
    
## AutoRec
一个自编码器（两层mlp），从m维向量到k维再到m维，用输入监督输出，中间的k维用作降维特征。由于输出损失存在，模型起到泛化作用。模型可以对缺失值进行填补。

加入正则化更好的避免过拟合。

> User-based和Item-based的区别

    · User-Based每个编码了一个用户对所有物品的评分，只需要forward一次用户向量就知道对所有物品的评分了
    · 但是用户向量是稀疏的，影响到了模型效果

# 特征交叉方法的探索
## Deep Crossing

> 方法

    · embedding层将稀疏特征转换为稠密特征（通过fc层，相当于查表）
    · stacking层将所有的特征拼接在一起
    · multi res layer：多层残差网络将特征进行充分的融合
    · scoring层通过softmax求解目标的概率（比如ctr）

> 历史意义

    · 从当时的角度看，充分的进行了特征的混合，没有手动进行特征交叉了（虽然是把特征拼起来，让resnet去做交叉，没有显式交叉）

## NeuralCF
深度学习的协同过滤模型。

> 方法

    · 将用户向量和物品向量通过fc层得到隐向量，并用隐向量点积得到得分，用sample进行优化
    · 将内积用多层神经网络替换进行输出，能让特征更加充分的混合
    · 可以同时将矩阵分解得到的隐向量一起融入进去

> 优劣

    · 更好的做了信息的交叉，更加灵活的进行了模型组合
    · 没有假如其他的context信息等，仍然是矩阵分解的套路


## PNN

> 方法

    · 相比与deep crossing，将stacking层改成了多个特征的组合拼接（内积外积），更早的进行了特征组合
    · PNN的外积池话实际上是使用了avg pooling后进行外积，损失信息
    
> 优劣

    · 使用了更多的特征，并且强调了主动的特征交叉，让模型更容易捕捉
    · 无差别的特征交叉可能容易损失有价值的信息
    
## Wide&Deep
Wide是单层模型，具有记忆能力；Deep是多层模型，具有泛化能力。记忆能力指的是看到相关的特征就输出相近似的结果，单层网络有这样的效果。
多层网络由于做了更多的特征组合，更好的泛化了预测。模型开创了不同网络融合的思路。

> 方法

    · 一部分的特征输入wide单层网络，一部分输入deep多层网络。因此如何选择特征输入的位置是关键
    · wide的部分输入的是直接与结果强相关的特征，充分利用网络的记忆能力；deep部分则是把所有的特征都输入，充分的挖掘相关性
    · wide的输入特征使用了交叉积变化，也包含了特征组合
    · 对两个网络实际上使用了不同的优化器，期望wide网络更稀疏，容易部署
    
## Deep&Cross(DCN)

> 方法

    · 用cross网络替代wide网络，提升特征的交互力度。多个交叉层，每个输入都会与原向量x0。
    · Deep网络更新成cross之后，抛弃了原来的多层mlp的结构，更加主动的进行了信息的融合（比如xix0的外积），使cross网络的表达能力更强


# 将FM融入网络结构
## FNN

>  要解决的问题
    
    · 隐含层参数量极大，难以收敛（仅有非零特征的权重被更新）
    · FNN将随即初始化的embedding用FM模型训练好的结果进行初始化。具体操作实际上是让隐权重向量v去初始化embedding层的连接权重（而不是结果）
    · 为特征预训练提供了思路
    
## DeepFM

> 方法

    · 用FM替换了deep & wide中的wide网络，和dcn一致。但是使用的是fm网络进行替换
    · 输入fm和deep网络中的embedding现在是一样的了
    
## NFM

> 方法

    · 用神经网络将FM的二阶特征交叉替换掉，争取更高阶的特征交叉形式
    · 特征交叉池话层
    
# 注意力机制的使用
## AFM

    · 在特征层和输出层之间加上一个注意力网络，将一个权重联系到特征上去作为注意力得分。