# 百面机器学习
## by 诸葛越

---
# 目录
#### [1.特征工程](#1)
#### [2.模型评估](#2)
#### [3.经典算法](#3)
#### [4.降维](#4)
#### [5.无监督学习](#5)

---

# <h1 id="1">第一章：特征工程</h1>

> 数值特征归一化

	· min-max: 归一化至0-1
	· z-score: x-mean / std, 正态分布
	· 归一化的目的是为了消除数值绝对值的影响，只考虑相对关系。
	· 从梯度的角度来说，如果值越大，梯度下降得越快，下降曲线曲折
	
> 类别型特征

    · 序号编码：用于有大小关系的数据，如高中低->123
    · onehot：稀疏向量节省空间，但是计算耗费大
    · 二进制编码：相当于multihot，比onehot空间更少
    
> 高维组合特征

    · wij<xi,xj>：维度数量为|xi|*|xj|. 如果取值特别多，维度空间可达O(mn).
    · 矩阵分解可以把wij分解为低维向量，维度降低为O((m+n)*k)
    
> 决策树进行特征组合

    · 用决策树对特征进行组合，判断样本从root到每条路径是否满足，是的话该路径特征为1，否则为0
    · 把所有的路径组合起来当作最后的特征
    
> 文本表示

    · TD-IFD=TF(t,d) * IDF(t)。IDF表示词的重要性，如果在多篇文章都出现过，说明词通用，并不重要。TF为词在文档d中的频率
    
> 数据不足

    · 模型方面：提升泛化性，简化模型/集成学习/dropout/正则化
    · 数据方面：数据增强-平移缩放裁剪。变换也可以在特征层面进行
    · 迁移学习：用更多数据训练大模型，在少量数据上finetune

# <h2 id="2">第二章：模型评估</h2>

> 指标的局限性

    · 准确率：当正负样本相差太大时，准确率容易变的很高
    · 准确与召回：当topN排序时，前面的N个都是正样本。precision要求保守，recall则相反，这两个有矛盾。要综合考虑两者
    · PR曲线：横轴recall，纵轴precision，曲线向下，说明成反比。PR曲线通过调整所有的阈值画出来
        · 召回率相等时，准确率越高说明排前面的都是正样本。需要考虑整个曲线的综合表现
        · F1 score = 2pr/(p+r) 
    · RMSE：当有离群点的时候，即使大部分时间误差都很小，指标可能也会很高
        · 过滤噪声
        · 对噪声建模
        · 采用MAPE，考虑相对偏移率，归一化误差
        
> ROC曲线

    · 横坐标FPR，纵坐标TPR。分子都是阳性的个数。真阳性可以理解为敏感性，是否能仔细挑出每一个正样本；横坐标为1-特异性
    · 当阈值无限大，说明全部都预测为负，为曲线上的原点
    · 曲线都位于y=x上方，否则只要将预测概率设置为1-p则是更好的分类器
    · 与PR曲线相比，受数据分布的影响更小，特别是正负样本相差极大时，更适合ROC曲线使用。但是对比在特定数据集上的性能，PR曲线更好
    
> 余弦相似度

    · 不关心两个向量的绝对大小，只考虑近似性。使用余弦还是欧式取决于你关注的问题
    · 余弦相似性在高维向量时仍表现的很好，但是欧式受维度的影响大（norm过向量则可以）
    · ||A-B|| = sqrt(2(1-cos(A,B))，norm过的向量有相互转换的关系
    · 余弦距离 1-余弦相似性。不满足三角不等式（等腰直角三角形的三点即可）
    
> A/B test

    · 线下环境通常无法完全还原线上的环境，同时无法消除线下数据过拟合的情况。有些指标只能在线上计算，因此仍然需要ab test
    · 采样需要无偏性和独立性
    
> 验证方法

    · holdout：固定一个采样，结果与采样随机性相关
    · k fold：分成k组，每次测试一组，取平均
    · 自主法：k次有放回的重复采样（无限大采样时，36.8%未被采样） 
    
> 降低过拟合

    · 更多的训练数据，更多的data aug
    · 降低模型复杂度，剪枝，dropout，集成学习
    · 正则化(l1 l2差别-l1拉普拉斯先验，更贴近0本身）
	· batch norm，early stop
	
> 提升欠拟合

    · 增加模型复杂度，引入更多/高阶特征

# <h3 id="3">第三章：经典算法</h3>

## 支持向量机
> 原理与推导
    · TODO

> 线性可分

    · 对于在超平面上分隔的两组点，他们投影在超平面上并不是线性可分的（两个点的case，投影在中垂线上相同）
    
## 逻辑回归
> 原理与推导

    · 逻辑回归是分类，线性回归是回归
    · p = sigmoid(wx+b) = 1 / (1 + e^-(wx+b))
    · wx + b = log(p/1-p)，发生与不发生的比率，是对y=1|x事件的线性回归
    · p(y|x, w) = (fw(x))^y * (1-fw(x))^(1-y)，极大似然估计f'(z) = f(z)*(1-f(z))*z
    
> softmax回归

    · 将y的二元分布改为多元
    · 也可以训练多个二元逻辑回归器进行多类别的判定
    
## 决策树
> 启发式函数

    · ID3：信息增益熵g(D,A) = H(D) - H(D|A). 熵增越大，分类特征越好
    · C4.5：信息增益比gr(D,A) = g(D,A) / Ha(D)
    · CAR：最大基尼系数，可用于回归任务，可重复利用特征，形成二叉树
    · ID3倾向于特征取值较多的（分类越细越好），但是泛化性能差。C4.5则对特征多的进行惩罚。
    
> 剪枝

    · 预剪枝：在构建树的过程中，如果无法带来信息增益或者树深度过大，则不再分裂
    · 后剪枝：先构建出决策树，通过剪枝与否判断树的性能，有增益再剪
    
# <h4 id="4">第四章：降维</h4>

> PCA

    · 信号具有较大方差，噪声具有较小方差，比值为信噪比。PCA就是要找到投影方差最大的方向
    · 将数据归一化之后，方差在w上表示为协方差的特征值。取前k个特征向量，对x进行投影，则为新的方向
    · 用最小二乘法优化投影后的方差，求解结果与最大方差法一致
    
> LDA

    · 相比于PCA的全局方差最小化，LDA希望类内距离小，类间距离大。同样可以用矩阵分解求解
    · 有类别标签的有监督投影
    
# <h5 id="5">第五章：无监督学习</h5>

> k-means

    · 随机选k个中心，对每个数据分到最近的中心，更新中心。重复过程直到稳定。复杂度是O(NKt)，t为迭代轮数
    · 受到初始值影响可能不能到最优解，不太适用于离散分类
    · 合理选择k值，判断对总体距离函数J的影响
    · 可以通过核函数映射到高维可分空间内
    
> k-means++

    · 选K的初始点的时候不再完全随机。希望K的中心点的距离越大越好
    
> isodata

    · 给定预期K值，过程中会通过最小样本数和最大方差控制每个簇的大小近为统一
    
> kmeans收敛性
    暂略
