# 百面机器学习
## by 诸葛越

---
# 目录
#### [1.特征工程](#1)
#### [2.模型评估](#2)
#### [3.经典算法](#3)
#### [4.降维](#4)
#### [5.无监督学习](#5)
#### [6.概率图模型](#6)
#### [7.优化算法](#7)
#### [8.采样](#8)
#### [12.集成学习](#12)
未包含的其余章节放到深度学习模块

---

# <h1 id="1">第一章：特征工程</h1>

> 数值特征归一化

	· min-max: 归一化至0-1
	· z-score: x-mean / std, 正态分布
	· 归一化的目的是为了消除数值绝对值的影响，只考虑相对关系。
	· 从梯度的角度来说，如果值越大，梯度下降得越快，下降曲线曲折
	
> 类别型特征

    · 序号编码：用于有大小关系的数据，如高中低->123
    · onehot：稀疏向量节省空间，但是计算耗费大
    · 二进制编码：相当于multihot，比onehot空间更少
    
> 高维组合特征

    · wij<xi,xj>：维度数量为|xi|*|xj|. 如果取值特别多，维度空间可达O(mn).
    · 矩阵分解可以把wij分解为低维向量，维度降低为O((m+n)*k)
    
> 决策树进行特征组合

    · 用决策树对特征进行组合，判断样本从root到每条路径是否满足，是的话该路径特征为1，否则为0
    · 把所有的路径组合起来当作最后的特征
    
> 文本表示

    · TD-IFD=TF(t,d) * IDF(t)。IDF表示词的重要性，如果在多篇文章都出现过，说明词通用，并不重要。TF为词在文档d中的频率
    
> 数据不足

    · 模型方面：提升泛化性，简化模型/集成学习/dropout/正则化
    · 数据方面：数据增强-平移缩放裁剪。变换也可以在特征层面进行
    · 迁移学习：用更多数据训练大模型，在少量数据上finetune

# <h2 id="2">第二章：模型评估</h2>

> 指标的局限性

    · 准确率：当正负样本相差太大时，准确率容易变的很高
    · 准确与召回：当topN排序时，前面的N个都是正样本。precision要求保守，recall则相反，这两个有矛盾。要综合考虑两者
    · PR曲线：横轴recall，纵轴precision，曲线向下，说明成反比。PR曲线通过调整所有的阈值画出来
        · 召回率相等时，准确率越高说明排前面的都是正样本。需要考虑整个曲线的综合表现
        · F1 score = 2pr/(p+r) 
    · RMSE：当有离群点的时候，即使大部分时间误差都很小，指标可能也会很高
        · 过滤噪声
        · 对噪声建模
        · 采用MAPE，考虑相对偏移率，归一化误差
        
> ROC曲线

    · 横坐标FPR，纵坐标TPR。分子都是阳性的个数。真阳性可以理解为敏感性，是否能仔细挑出每一个正样本；横坐标为1-特异性
    · 当阈值无限大，说明全部都预测为负，为曲线上的原点
    · 曲线都位于y=x上方，否则只要将预测概率设置为1-p则是更好的分类器
    · 与PR曲线相比，受数据分布的影响更小，特别是正负样本相差极大时，更适合ROC曲线使用。但是对比在特定数据集上的性能，PR曲线更好
    
> 余弦相似度

    · 不关心两个向量的绝对大小，只考虑近似性。使用余弦还是欧式取决于你关注的问题
    · 余弦相似性在高维向量时仍表现的很好，但是欧式受维度的影响大（norm过向量则可以）
    · ||A-B|| = sqrt(2(1-cos(A,B))，norm过的向量有相互转换的关系
    · 余弦距离 1-余弦相似性。不满足三角不等式（等腰直角三角形的三点即可）
    
> A/B test

    · 线下环境通常无法完全还原线上的环境，同时无法消除线下数据过拟合的情况。有些指标只能在线上计算，因此仍然需要ab test
    · 采样需要无偏性和独立性
    
> 验证方法

    · holdout：固定一个采样，结果与采样随机性相关
    · k fold：分成k组，每次测试一组，取平均
    · 自主法：k次有放回的重复采样（无限大采样时，36.8%未被采样） 
    
> 降低过拟合

    · 更多的训练数据，更多的data aug
    · 降低模型复杂度，剪枝，dropout，集成学习
    · 正则化(l1 l2差别-l1拉普拉斯先验，更贴近0本身）
	· batch norm，early stop
	
> 提升欠拟合

    · 增加模型复杂度，引入更多/高阶特征

# <h3 id="3">第三章：经典算法</h3>

## 支持向量机
> 原理与推导
    · TODO

> 线性可分

    · 对于在超平面上分隔的两组点，他们投影在超平面上并不是线性可分的（两个点的case，投影在中垂线上相同）
    
## 逻辑回归
> 原理与推导

    · 逻辑回归是分类，线性回归是回归
    · p = sigmoid(wx+b) = 1 / (1 + e^-(wx+b))
    · wx + b = log(p/1-p)，发生与不发生的比率，是对y=1|x事件的线性回归
    · p(y|x, w) = (fw(x))^y * (1-fw(x))^(1-y)，极大似然估计f'(z) = f(z)*(1-f(z))*z
    
> softmax回归

    · 将y的二元分布改为多元
    · 也可以训练多个二元逻辑回归器进行多类别的判定
    
## 决策树
> 启发式函数

    · ID3：信息增益熵g(D,A) = H(D) - H(D|A). 熵增越大，分类特征越好
    · C4.5：信息增益比gr(D,A) = g(D,A) / Ha(D)
    · CAR：最大基尼系数，可用于回归任务，可重复利用特征，形成二叉树
    · ID3倾向于特征取值较多的（分类越细越好），但是泛化性能差。C4.5则对特征多的进行惩罚。
    
> 剪枝

    · 预剪枝：在构建树的过程中，如果无法带来信息增益或者树深度过大，则不再分裂
    · 后剪枝：先构建出决策树，通过剪枝与否判断树的性能，有增益再剪
    
# <h4 id="4">第四章：降维</h4>

> PCA

    · 信号具有较大方差，噪声具有较小方差，比值为信噪比。PCA就是要找到投影方差最大的方向
    · 将数据归一化之后，方差在w上表示为协方差的特征值。取前k个特征向量，对x进行投影，则为新的方向
    · 用最小二乘法优化投影后的方差，求解结果与最大方差法一致
    
> LDA

    · 相比于PCA的全局方差最小化，LDA希望类内距离小，类间距离大。同样可以用矩阵分解求解
    · 有类别标签的有监督投影
    
# <h5 id="5">第五章：无监督学习</h5>

> k-means

    · 随机选k个中心，对每个数据分到最近的中心，更新中心。重复过程直到稳定。复杂度是O(NKt)，t为迭代轮数
    · 受到初始值影响可能不能到最优解，不太适用于离散分类
    · 合理选择k值，判断对总体距离函数J的影响
    · 可以通过核函数映射到高维可分空间内
    
> k-means++

    · 选K的初始点的时候不再完全随机。希望K的中心点的距离越大越好
    
> isodata

    · 给定预期K值，过程中会通过最小样本数和最大方差控制每个簇的大小近为统一
    
> kmeans收敛性
    暂略

> 高斯混合分布

    · 有多个类，每个类都符合高斯分布。数据是从多个高斯分布进行混合生成的。需要估计每个高斯的均值和方差，加上混合的权重
    · EM算法预测：初始参数，根据当前参数计算每一个点生成的概率；用概率调整高斯参数和权重
    · 相比于kmeans优势是可以生成样本点，并且给出样本的生成概率
    
> 聚类算法评估

    · 估计聚类趋势：如果随着K增加，整体误差越小，则存在非随机的簇结构
    · 霍普金斯统计量：样本中抽取最近距离，和随机生成最近距离的差距。如果聚类趋势明显，抽取的最近距离小，统计量H~=1
    · 判定最优的K
    · 测定质量：轮廓系数、均方根平方差、Rsquare等等
    
# <h6 id="6">第六章：概率图模型</h6>

> 朴素贝叶斯

    · p(y|x) = p(x|y)p(y) / p(x), 如果为某一类，样本展现的x概率有多大。借此判断x对应的类别y
    
> 最大熵模型

    · 熵：H(p) = -sum P(x)logP(x). x为均匀分布时熵最大-不确定性最高
    · 最大熵模型学习w使得pw(y|x)最大
    
> 生成式模型和判别式模型

    · 生成式：先对联合概率建模，再通过边缘概率计算求解（贝叶斯系列，隐马尔可夫，LDA都是如此）
    · 判别式：不需要联合建模（最大熵模型，条件随机场）
    
> 主题模型
暂略
>

# <h7 id="7">第七章：优化算法</h7>

> 损失函数

    · 01损失：1[f(x)y<=0]
    · hinge损失： max(0, 1-f(x)y)，在f(x)y=1处不可导
    · logistic损失： log(1+exp(-fy))
    · cross entropy：-log(1+fy / 2)
    · 平方损失: (f-y)^2, 对异常点比较敏感
    · 绝对损失：|f-y|, 更稳定但是边界无法求导
    · huber loss：|f-y|较小时平方损失，较大时线性损失，处处可导
    
> 凸函数

    · 函数上任意两点的线段都不会低于函数曲线下方
    · 二阶hessian矩阵为半正定则函数为凸
    · 凸函数局部最小则是全局最小
    
> 优化算法

    · 直接法：需要函数为凸且有闭式解
    · 二阶法牛顿法：需要求二阶hessian的逆矩阵，复杂度大
    · 二阶：xn+1 = xn - f(xn) / f`(xn)

> 方法及优势

    · BGD：把全部的cost集合起来算grad，计算速度慢，更新慢，但是梯度稳定
    · SGD: 每一个样本都算一次，更新梯度快。但是不稳定
    · MSGD: 每个batch更新一次，介于BGD和SGD之间，容易陷入鞍点
    动量优化：
    · SGD+Momentum：动态保留上一个梯度方向，保证梯度更新方向较稳定，收敛更快
    自适应学习率：
    · Nesterov Acce Grad：预测未来的梯度，与动量结合进行更新
    · Adagrad：根据梯度的积累动态调整学习率（独立每个参数），越大的梯度累计更新的慢。但是最后梯度积累容易导致更新停止
    · Adadelta：相对于adagrad，不用历史的综合，而是用一个动态的衰减梯度平方和，让最后的梯度不会到无限大。实际使用不需要学习率了。
    · RMSprop：形式上和adadelta基本差不多，设定了动态保留率为0.9
    · Adam：既要通过momentum修正梯度的方向，又要结合动态的梯度平方和调整修正的大小。通常收敛较快，效果比较好
    
> L1正则和L2正则的三个角度

    · 稀疏化是为了线上推断时节省空间和时间，同时降低过拟合
    · 三个方向解释稀疏的程度
        · 圆形和菱形解空间与loss函数的交点
        · 函数加上正则后导数的效果
        · 先验的拉普拉斯分布和正态分布

# <h8 id="8">第八章：采样</h8>

> 伪随机数

    · xn+1 = a * xn + c(mod m)生成[0,m-1]上的随机数
    · 需要精心挑选m/a/c以防循环周期出现
    
> 从均匀采样到高斯采样

    · 通过逆函数采样。对于高斯来说需要xy同时采样
    
> MCMC和贝叶斯采样

    · 暂略
    

# <h12 id="12">第十二章：集成学习</h12>