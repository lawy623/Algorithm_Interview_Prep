# 百面机器学习
## by 诸葛越

---
# 目录
#### [1.特征工程](#1)
#### [2.模型评估](#2)

---

# <h1 id="1">第一章：特征工程</h1>

> 数值特征归一化

	· min-max: 归一化至0-1
	· z-score: x-mean / std, 正态分布
	· 归一化的目的是为了消除数值绝对值的影响，只考虑相对关系。
	· 从梯度的角度来说，如果值越大，梯度下降得越快，下降曲线曲折
	
> 类别型特征

    · 序号编码：用于有大小关系的数据，如高中低->123
    · onehot：稀疏向量节省空间，但是计算耗费大
    · 二进制编码：相当于multihot，比onehot空间更少
    
> 高维组合特征

    · wij<xi,xj>：维度数量为|xi|*|xj|. 如果取值特别多，维度空间可达O(mn).
    · 矩阵分解可以把wij分解为低维向量，维度降低为O((m+n)*k)
    
> 决策树进行特征组合

    · 用决策树对特征进行组合，判断样本从root到每条路径是否满足，是的话该路径特征为1，否则为0
    · 把所有的路径组合起来当作最后的特征
    
> 文本表示

    · TD-IFD=TF(t,d) * IDF(t)。IDF表示词的重要性，如果在多篇文章都出现过，说明词通用，并不重要。TF为词在文档d中的频率
    
> 数据不足

    · 模型方面：提升泛化性，简化模型/集成学习/dropout/正则化
    · 数据方面：数据增强-平移缩放裁剪。变换也可以在特征层面进行
    · 迁移学习：用更多数据训练大模型，在少量数据上finetune

# <h2 id="2">第二章：模型评估</h2>

> 指标的局限性

    · 准确率：当正负样本相差太大时，准确率容易变的很高
    · 准确与召回：当topN排序时，前面的N个都是正样本。precision要求保守，recall则相反，这两个有矛盾。要综合考虑两者
    · PR曲线：横轴recall，纵轴precision，曲线向下，说明成反比。PR曲线通过调整所有的阈值画出来
        · 召回率相等时，准确率越高说明排前面的都是正样本。需要考虑整个曲线的综合表现
        · F1 score = 2pr/(p+r) 
    · RMSE：当有离群点的时候，即使大部分时间误差都很小，指标可能也会很高
        · 过滤噪声
        · 对噪声建模
        · 采用MAPE，考虑相对偏移率，归一化误差
        
> ROC曲线

    · 横坐标FPR，纵坐标TPR。分子都是阳性的个数。真阳性可以理解为敏感性，是否能仔细挑出每一个正样本；横坐标为1-特异性
    · 当阈值无限大，说明全部都预测为负，为曲线上的原点
    · 曲线都位于y=x上方，否则只要将预测概率设置为1-p则是更好的分类器
    · 与PR曲线相比，受数据分布的影响更小，特别是正负样本相差极大时，更适合ROC曲线使用。但是对比在特定数据集上的性能，PR曲线更好
    
> 余弦相似度

    · 不关心两个向量的绝对大小，只考虑近似性。使用余弦还是欧式取决于你关注的问题
    · 余弦相似性在高维向量时仍表现的很好，但是欧式受维度的影响大（norm过向量则可以）
    · ||A-B|| = sqrt(2(1-cos(A,B))，norm过的向量有相互转换的关系
    · 余弦距离 1-余弦相似性。不满足三角不等式（等腰直角三角形的三点即可）
    
> A/B test

    · 线下环境通常无法完全还原线上的环境，同时无法消除线下数据过拟合的情况。有些指标只能在线上计算，因此仍然需要ab test
    · 采样需要无偏性和独立性
    
> 验证方法

    · holdout：固定一个采样，结果与采样随机性相关
    · k fold：分成k组，每次测试一组，取平均
    · 自主法：k次有放回的重复采样（无限大采样时，36.8%未被采样） 
    
> 降低过拟合

    · 更多的训练数据，更多的data aug
    · 降低模型复杂度，剪枝，dropout，集成学习
    · 正则化(l1 l2差别-l1拉普拉斯先验，更贴近0本身）
	· batch norm，early stop
	
> 提升欠拟合

    · 增加模型复杂度，引入更多/高阶特征